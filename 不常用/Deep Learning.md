# nn.ModuleList和nn.Sequential

 - nn.Sequential内部实现了forward函数，因此可以不用写forward函数，而nn.ModuleList则没有实现内部forward函数。
 - nn.Sequential可以使用OrderedDict对每层进行命名。
 -  nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而nn.ModuleList并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言。

## nn.ModuleList
不同于一般的list，加入到nn.ModuleList里面的module是会自动注册到整个网络上的，同时module的parameters也会自动添加到整个网络中。
## 忠告
如果确定 nn.Sequential 里面的顺序是你想要的，而且不需要再添加一些其他处理的函数 (比如 nn.functional 里面的函数 )，那么完全可以直接用 nn.Sequential。这么做的代价就是失去了**部分灵活性**，毕竟不能自己去定制 forward 函数里面的内容了。

# nn.Embedding

```python
import torch
from torch import nn
 
# 创建最大词个数为10，每个词用维度为4表示
embedding = nn.Embedding(10, 4)
 
# 将第一个句子填充0，与第二个句子长度对齐
in_vector = torch.LongTensor([[1, 2, 3, 4, 0, 0], [1, 2, 5, 6, 5, 7]])
out_emb = embedding(in_vector)
print(in_vector.shape)
print((out_emb.shape))
print(out_emb)
print(embedding.weight)
```

为什么输入向量是2乘6的矩阵，而embedding的权重是10乘4的，还可以进行embedding？

在PyTorch中，`nn.Embedding`类是用来将离散的数字（通常用来表示词语或者其它类型的标记）转换成向量的。这些数字每一个都对应于嵌入矩阵中的一行。嵌入矩阵的行数代表可嵌入的最大词汇量，而每一行的维度代表了嵌入向量的维度。

当你创建一个嵌入层 `nn.Embedding(10, 4)`时，你告诉PyTorch你想要一个能够嵌入最多10个不同的词的矩阵，每一个词被表示为一个4维的向量。这就是为什么嵌入的权重大小是10乘4的原因——你有10个可能的输入词，每个词被转换为一个4维的嵌入向量。

在这个例子中，你有两个句子，每个句子由6个词的索引组成。这些索引用来从嵌入矩阵中选取相应的向量。因此，尽管你的输入矩阵是2乘6的，每个索引都会被转换成一个4维的向量，所以输出的嵌入向量的维度将会是2乘6乘4。这里的2代表句子数量，6代表每个句子的词数，4代表每个嵌入向量的维度。

**嵌入操作实际上是在做索引操作：它查看输入的索引，并从嵌入矩阵中选取对应行的向量。这就是为什么即使输入向量的维度和嵌入矩阵的维度不直接相关，嵌入操作仍然可以工作的原因。**

**注：embeddings中的值是正态分布N(0,1)中随机取值。**

索引操作！！！！！！！！！

```python
torch.Size([2, 6])
torch.Size([2, 6, 4])
tensor([[[-0.6642, -0.6263,  1.2333, -0.6055],
         [ 0.9950, -0.2912,  1.0008,  0.1202],
         [ 1.2501,  0.1923,  0.5791, -1.4586],
         [-0.6935,  2.1906,  1.0595,  0.2089],
         [ 0.7359, -0.1194, -0.2195,  0.9161],
         [ 0.7359, -0.1194, -0.2195,  0.9161]],
 
        [[-0.6642, -0.6263,  1.2333, -0.6055],
         [ 0.9950, -0.2912,  1.0008,  0.1202],
         [-0.3216,  1.2407,  0.2542,  0.8630],
         [ 0.6886, -0.6119,  1.5270,  0.1228],
         [-0.3216,  1.2407,  0.2542,  0.8630],
         [ 0.0048,  1.8500,  1.4381,  0.3675]]], grad_fn=<EmbeddingBackward0>)
Parameter containing:
tensor([[ 0.7359, -0.1194, -0.2195,  0.9161],
        [-0.6642, -0.6263,  1.2333, -0.6055],
        [ 0.9950, -0.2912,  1.0008,  0.1202],
        [ 1.2501,  0.1923,  0.5791, -1.4586],
        [-0.6935,  2.1906,  1.0595,  0.2089],
        [-0.3216,  1.2407,  0.2542,  0.8630],
        [ 0.6886, -0.6119,  1.5270,  0.1228],
        [ 0.0048,  1.8500,  1.4381,  0.3675],
        [ 0.3810, -0.7594, -0.1821,  0.5859],
        [-1.4029,  1.2243,  0.0374, -1.0549]], requires_grad=True)
```

